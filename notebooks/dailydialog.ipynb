{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DailyDialog"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2) dialogues_topic.txt:  {1: Ordinary Life, 2: School Life, 3: Culture & Education,\n",
    "                        4: Attitude & Emotion, 5: Relationship, 6: Tourism , 7: Health, 8: Work, 9: Politics, 10: Finance}\n",
    "\n",
    "3) dialogues_act.txt: { 1: inform，2: question, 3: directive, 4: commissive }\n",
    "\n",
    "4) dialogues_emotion.txt: { 0: no emotion, 1: anger, 2: disgust, 3: fear, 4: happiness, 5: sadness, 6: surprise}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://yanran.li/dailydialog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tools"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь чисто текстовый файл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The kitchen stinks . __eou__ I'll throw out the garbage . __eou__\\nSo Dick , how about getting some coffee for tonight ? __eou__ Coffee ? I don ’ t honestly like that kind of stuff . __eou__ Come on , \""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"/workspaces/codespaces-jupyter/data/ijcnlp_dailydialog/dialogues_text.txt\", \"r\") as f:\n",
    "    raw_data = f.read()\n",
    "raw_data[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The kitchen stinks . ',\n",
       " \" I'll throw out the garbage . \",\n",
       " 'So Dick , how about getting some coffee for tonight ? ',\n",
       " ' Coffee ? I don ’ t honestly like that kind of stuff . ',\n",
       " ' Come on , you can at least try a little , besides your cigarette . ',\n",
       " ' What ’ s wrong with that ? Cigarette is the thing I go crazy for . ',\n",
       " ' Not for me , Dick . ',\n",
       " 'Are things still going badly with your houseguest ? ',\n",
       " ' Getting worse . Now he ’ s eating me out of house and home . I ’ Ve tried talking to him but it all goes in one ear and out the other . He makes himself at home , which is fine . But what really gets me is that yesterday he walked into the living room in the raw and I had company over ! That was the last straw . ',\n",
       " ' Leo , I really think you ’ re beating around the bush with this guy . I know he used to be your best friend in college , but I really think it ’ s time to lay down the law . ']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = [x.split(\"__eou__\") for x in raw_data.split(\"\\n\")]\n",
    "q = []\n",
    "for i in s:\n",
    "    q.extend(i)\n",
    "dataset = [i for i in q if i != '']\n",
    "dataset[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102980"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk 12 completed\n",
      "chunk 13 completed\n",
      "chunk 14 completed\n",
      "chunk 15 completed\n",
      "chunk 16 completed\n",
      "chunk 17 completed\n",
      "chunk 18 completed\n",
      "chunk 19 completed\n",
      "chunk 20 completed\n",
      "chunk 21 completed\n",
      "chunk 22 completed\n",
      "chunk 23 completed\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m chank_num \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m12\u001b[39m, \u001b[39mlen\u001b[39m(dataset)\u001b[39m/\u001b[39m\u001b[39m/\u001b[39m\u001b[39m1000\u001b[39m):\n\u001b[1;32m      3\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m----> 4\u001b[0m         date_sentces \u001b[39m=\u001b[39m tools\u001b[39m.\u001b[39;49mextract_date_roots(dataset[chank_num\u001b[39m*\u001b[39;49m\u001b[39m1000\u001b[39;49m:(chank_num\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m)\u001b[39m*\u001b[39;49m\u001b[39m1000\u001b[39;49m])\n\u001b[1;32m      5\u001b[0m         \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m date_sentces:\n\u001b[1;32m      6\u001b[0m             \u001b[39mif\u001b[39;00m (i[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39mNone\u001b[39;00m):\n",
      "File \u001b[0;32m/workspaces/codespaces-jupyter/notebooks/tools.py:26\u001b[0m, in \u001b[0;36mextract_date_roots\u001b[0;34m(sents)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     filtered_tok_seq \u001b[39m=\u001b[39m [] \u001b[39m# filter extra dependencies of root word\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m     head_tok_seq \u001b[39m=\u001b[39m [\u001b[39m*\u001b[39m[i \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m token_head\u001b[39m.\u001b[39mlefts \u001b[39mif\u001b[39;00m i \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m tok_seq], token_head, \u001b[39m*\u001b[39m[i \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m token_head\u001b[39m.\u001b[39mrights \u001b[39mif\u001b[39;00m i \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m tok_seq]]\n\u001b[1;32m     27\u001b[0m     \u001b[39mfor\u001b[39;00m j, seqtok \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(head_tok_seq):\n\u001b[1;32m     28\u001b[0m         \u001b[39mif\u001b[39;00m (seqtok\u001b[39m.\u001b[39mdep_ \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mdobj\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mnsubj\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mnummod\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mROOT\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mneg\u001b[39m\u001b[39m\"\u001b[39m]) \u001b[39mor\u001b[39;00m (seqtok \u001b[39mis\u001b[39;00m token_head):\n",
      "File \u001b[0;32m/workspaces/codespaces-jupyter/notebooks/tools.py:26\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     filtered_tok_seq \u001b[39m=\u001b[39m [] \u001b[39m# filter extra dependencies of root word\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m     head_tok_seq \u001b[39m=\u001b[39m [\u001b[39m*\u001b[39m[i \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m token_head\u001b[39m.\u001b[39mlefts \u001b[39mif\u001b[39;00m i \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m tok_seq], token_head, \u001b[39m*\u001b[39m[i \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m token_head\u001b[39m.\u001b[39mrights \u001b[39mif\u001b[39;00m i \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m tok_seq]]\n\u001b[1;32m     27\u001b[0m     \u001b[39mfor\u001b[39;00m j, seqtok \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(head_tok_seq):\n\u001b[1;32m     28\u001b[0m         \u001b[39mif\u001b[39;00m (seqtok\u001b[39m.\u001b[39mdep_ \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mdobj\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mnsubj\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mnummod\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mROOT\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mneg\u001b[39m\u001b[39m\"\u001b[39m]) \u001b[39mor\u001b[39;00m (seqtok \u001b[39mis\u001b[39;00m token_head):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data = []\n",
    "for chank_num in range(0, len(dataset)//1000):\n",
    "    try:\n",
    "        date_sentces = tools.extract_date_roots(dataset[chank_num*1000:(chank_num+1)*1000])\n",
    "        for i in date_sentces:\n",
    "            if (i[1] == None):\n",
    "                data.append([i[0], i[1], i[2], 0])\n",
    "            else:\n",
    "                data.append([i[0], i[1], i[2], 1])\n",
    "        print(f\"chunk {chank_num} completed\")\n",
    "    except Exception as e:\n",
    "        print(f\"chunk failed {chank_num}. Exception: {e}\")\n",
    "data_df = pd.DataFrame(data, columns=[\"text\", \"root\", \"event\", \"label\"])\n",
    "data_df.to_csv(\"processed_data/dailydialog_datesentences.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
